{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32f12e-0bc0-4efc-a53d-d92facac565a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'diabetes_CDC.csv'\n",
    "d_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(d_df.head())\n",
    "d_df.columns\n",
    "d_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bff259-94a0-4c59-a6d8-4f6c530b120e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values_count = d_df.isnull().sum()\n",
    "print(\"Missing values count:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values > 0\n",
    "missing_columns = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values > 0\n",
    "if not missing_columns.empty:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_columns)\n",
    "else:\n",
    "    print(\"No missing values found in any column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccf7da-be1f-4b86-ae93-7b489cecc69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the data types of all variables\n",
    "variable_types = d_df.dtypes\n",
    "\n",
    "# Print the data types\n",
    "print(\"Data types of all variables:\")\n",
    "print(variable_types)\n",
    "\n",
    "# Binary variables represented as integers (0 or 1) can be used as int type.\n",
    "# Converting Catrgorical Variable types from int to category\n",
    "\n",
    "# Convert ordinal variables to categorical factors\n",
    "d_df['MentHlth'] = d_df['MentHlth'].astype('category')\n",
    "d_df['PhysHlth'] = d_df['PhysHlth'].astype('category')\n",
    "d_df['Education'] = d_df['Education'].astype('category')\n",
    "d_df['Income'] = d_df['Income'].astype('category')\n",
    "d_df['Age'] = d_df['Age'].astype('category')\n",
    "# Replace values in 'GenHlth' column with their inverses\n",
    "d_df['GenHlth'] = d_df['GenHlth'].replace({1: 5, 2: 4, 3: 3, 4: 2, 5: 1})\n",
    "\n",
    "# Display the modified 'GenHlth' column\n",
    "print(d_df['GenHlth'])\n",
    "\n",
    "print(d_df['GenHlth'].head())\n",
    "d_df['GenHlth'] = d_df['GenHlth'].astype('category')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(d_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8b0dd-7f5a-4eb8-be60-14b3034d7b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the data types of all variables\n",
    "variable_types = d_df.dtypes\n",
    "\n",
    "# Print the data types\n",
    "print(\"Data types of all variables:\")\n",
    "print(variable_types)\n",
    "print(d_df['GenHlth'])\n",
    "\n",
    "\n",
    "# Assuming d_df is your DataFrame and it contains the following columns\n",
    "selected_columns = ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP', 'target']\n",
    "\n",
    "# Extract the relevant columns\n",
    "selected_df = d_df[selected_columns]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "selected_df.to_csv('selected_features.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae93c34-6077-4493-b6a0-352c8b50c257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get unique values of the target variable\n",
    "unique_targets = d_df['target'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique values of the target variable:\")\n",
    "print(unique_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c3503-eb01-427a-8365-182f72e95ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d_df.corr(numeric_only=True)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool_))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "x = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Show Plot\n",
    "print(\"---CORRELATIONS---\")\n",
    "print(corr)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043333d-9209-4b62-8879-02236aea047e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = d_df.corr(numeric_only=True)\n",
    "\n",
    "# Get the correlation values with the target variable\n",
    "target_corr = corr['target'].drop('target')\n",
    "\n",
    "# Print the correlation values with the target variable\n",
    "print(\"Correlation with target variable:\")\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfc2f7-4c20-4596-9497-646565a533f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold object with a threshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "# Fit and transform the data to remove low variance features\n",
    "X_high_variance = selector.fit_transform(d_df.drop(columns=['target']))\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = d_df.drop(columns=['target']).columns[selector.get_support()]\n",
    "\n",
    "\n",
    "# Print the total number of features\n",
    "total_features = len(d_df.drop(columns=['target']).columns)\n",
    "print(\"Total features:\", total_features)\n",
    "\n",
    "# Print the count of selected features\n",
    "selected_features_count = len(selected_features)\n",
    "print(\"Number of selected features after applying low variance filter:\", selected_features_count)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features after applying low variance filter:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142364f5-fa22-41e0-907e-c200355ff612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold object with a threshold\n",
    "selector = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "# Fit and transform the data to remove low variance features\n",
    "X_high_variance = selector.fit_transform(d_df.drop(columns=['target']))\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = d_df.drop(columns=['target']).columns[selector.get_support()]\n",
    "\n",
    "\n",
    "# Print the total number of features\n",
    "total_features = len(d_df.drop(columns=['target']).columns)\n",
    "print(\"Total features:\", total_features)\n",
    "\n",
    "# Print the count of selected features\n",
    "selected_features_count = len(selected_features)\n",
    "print(\"Number of selected features after applying low variance filter:\", selected_features_count)\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features after applying low variance filter:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecd330-55d6-4cc7-966e-973e5c8cbc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature selection using SelectFromModel with RandomForestClassifier\n",
    "# This method selects features based on their importance as determined by a Random Forest Classifier.\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define your dataset (features and target)\n",
    "feat = d_df.drop(columns=['target'])\n",
    "tar = d_df['target']\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=3, criterion='entropy', random_state=50)\n",
    "\n",
    "#LogisticRegression(max_iter=1000, random_state=50)\n",
    "\n",
    "# Create the SelectFromModel object\n",
    "sel = SelectFromModel(estimator=clf, prefit=False, threshold='mean')\n",
    "\n",
    "# Fit the SelectFromModel object to the dataset\n",
    "sel.fit(feat, tar)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = feat.columns[sel.get_support()]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected features using SelectFromModel with Random Forest Classifier:\")\n",
    "print(\"Total features:\", len(feat.columns))\n",
    "print(\"Number of selected features:\", len(selected_features))\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa50546-7a55-4568-b4cb-db29622fb8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assigning features based on the two feature selection methods\n",
    "selected_features_rf = ['HighBP', 'BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "selected_features_lv = ['HighBP', 'HighChol', 'BMI', 'Smoker', 'PhysActivity', 'Fruits',\n",
    "                        'Veggies', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age',\n",
    "                        'Education', 'Income']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53d544-680e-410a-9e72-ce076f7ff924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Desciprtive Statistics\n",
    "import pandas as pd\n",
    "\n",
    "# Select the subset of data containing only the selected features\n",
    "selected_features_data = d_df[selected_features_rf]\n",
    "print(selected_features_data.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb050572-e583-4ff4-9bd5-a897773329b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot bar plot for 'GenHlth'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='GenHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of General Health', fontsize=16)\n",
    "plt.xlabel('General Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'MentHlth'\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(x='MentHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Mental Health', fontsize=16)\n",
    "plt.xlabel('Mental Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'PhysHlth'\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.countplot(x='PhysHlth', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Physical Health', fontsize=16)\n",
    "plt.xlabel('Physical Health', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'Education'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Education', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Education', fontsize=16)\n",
    "plt.xlabel('Education', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plot for 'Income'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Income', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Income', fontsize=16)\n",
    "plt.xlabel('Income', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot bar plot for 'HighBP'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='HighBP', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of High Blood Pressure', fontsize=16)\n",
    "plt.xlabel('High Blood Pressure', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks([0, 1], ['No', 'Yes'], fontsize=12)  # Replace 0 and 1 with labels\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot bar plot for 'Age'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Age', data=selected_features_data, palette='pastel')\n",
    "plt.title('Distribution of Age', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for 'BMI'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=selected_features_data, x='BMI', bins=20, color='skyblue')\n",
    "plt.title('Distribution of BMI', fontsize=16)\n",
    "plt.xlabel('BMI', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d443a2-3c39-49ee-a4b6-f1f11ab9a133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Split the Data Train and Test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# all features\n",
    "\n",
    "# Separate features and target\n",
    "diabetes_X = d_df.drop(columns=['target'])\n",
    "print(diabetes_X.columns)\n",
    "diabetes_Y = d_df['target'] \n",
    "print(diabetes_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(diabetes_X, diabetes_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "# selected_features_rf\n",
    "# Separate features and target\n",
    "rf_X = d_df[selected_features_rf]\n",
    "print(rf_X.columns)\n",
    "rf_Y = d_df['target'] \n",
    "print(rf_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(rf_X, rf_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train_rf.shape, X_test_rf.shape, Y_train_rf.shape, Y_test_rf.shape)\n",
    "\n",
    "# selected_features_lv\n",
    "# Separate features and target\n",
    "lv_X = d_df[selected_features_lv]\n",
    "print(lv_X.columns)\n",
    "lv_Y = d_df['target'] \n",
    "print(lv_Y.head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_lv, X_test_lv, Y_train_lv, Y_test_lv = train_test_split(lv_X, lv_Y, test_size=0.2, random_state=55)\n",
    "# Display the shape of the training and testing sets\n",
    "print(X_train_lv.shape, X_test_lv.shape, Y_train_lv.shape, Y_test_lv.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe24eb2-c4d9-4c8c-bfb5-0618622b7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "# Logistic Regression on three sets of features\n",
    "\n",
    "# all features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_all = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_all.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg = logreg_all.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred_logreg))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(Y_test, y_pred_logreg)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision = precision_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall = recall_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score = f1_score(Y_test, y_pred_logreg, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy:\", overall_accuracy)\n",
    "print(\"Overall Precision:\", overall_precision)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "print(\"Overall F1-Score:\", overall_f1_score)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred_logreg)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_rf\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_rf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_rf.fit(X_train_rf, Y_train_rf)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_rf = logreg_rf.predict(X_test_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Random Forest:\\n\", classification_report(Y_test_rf, y_pred_logreg_rf))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_rf = accuracy_score(Y_test_rf, y_pred_logreg_rf)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_rf = precision_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_rf = recall_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_rf = f1_score(Y_test_rf, y_pred_logreg_rf, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Random Forest:\", overall_accuracy_rf)\n",
    "print(\"Overall Precision for selected features using Random Forest:\", overall_precision_rf)\n",
    "print(\"Overall Recall for selected features using Random Forest:\", overall_recall_rf)\n",
    "print(\"Overall F1-Score for selected features using Random Forest:\", overall_f1_score_rf)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(Y_test_rf, y_pred_logreg_rf)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf = pd.DataFrame(conf_matrix_rf, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Random Forest:\")\n",
    "print(confusion_df_rf)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_lv\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_lv = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_lv.fit(X_train_lv, Y_train_lv)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_lv = logreg_lv.predict(X_test_lv)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Low Variance:\\n\", classification_report(Y_test_lv, y_pred_logreg_lv))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_lv = accuracy_score(Y_test_lv, y_pred_logreg_lv)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_lv = precision_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_lv = recall_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_lv = f1_score(Y_test_lv, y_pred_logreg_lv, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Low Variance:\", overall_accuracy_lv)\n",
    "print(\"Overall Precision for selected features using Low Variance:\", overall_precision_lv)\n",
    "print(\"Overall Recall for selected features using Low Variance:\", overall_recall_lv)\n",
    "print(\"Overall F1-Score for selected features using Low Variance:\", overall_f1_score_lv)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_lv = confusion_matrix(Y_test_lv, y_pred_logreg_lv)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv = pd.DataFrame(conf_matrix_lv, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Low Variance:\")\n",
    "print(confusion_df_lv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881086f-7591-4631-a47f-148522d7d8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "\n",
    "# Calculate the proportion of each class in the target variable\n",
    "class_counts = d_df['target'].value_counts()\n",
    "class_proportions = class_counts / class_counts.sum()\n",
    "\n",
    "# Print the class proportions\n",
    "print(\"Class Proportions:\")\n",
    "print(class_proportions)\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbdfb5-492c-4b1b-9f13-047134ed5c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade imbalanced-learn scikit-learn\n",
    "#pip install -U scikit-learn imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185fec2-8a10-42c5-9d2f-d4bcf23292f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe97847-228c-40c1-aa06-3304eb444828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Resampling using SMOTE to avoid class imbalance\n",
    "# Apply SMOTE to the training data only\n",
    "\n",
    "# all features\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
    "print(X_train.shape, Y_train.shape, X_train_resampled.shape,Y_train_resampled.shape)\n",
    "\n",
    "# Checking the Resampled Counts\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train_resampled and Y_train_resampled are NumPy arrays or pandas DataFrames/Series\n",
    "# Convert them to pandas DataFrame/Series if they are not already\n",
    "\n",
    "# Convert to DataFrame if they are numpy arrays\n",
    "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "# Convert to Series if they are numpy arrays\n",
    "Y_train_resampled_series = pd.Series(Y_train_resampled)\n",
    "\n",
    "# Create a DataFrame with X_train_resampled and Y_train_resampled\n",
    "resampled_df = pd.concat([X_train_resampled_df, Y_train_resampled_series], axis=1)\n",
    "\n",
    "# Calculate the proportion of each class in the target variable for resampled data\n",
    "resampled_class_counts = resampled_df['target'].value_counts()\n",
    "resampled_class_proportions = resampled_class_counts / resampled_class_counts.sum()\n",
    "\n",
    "# Print the class counts and proportions for resampled data\n",
    "print(\"Resampled Class Counts:\")\n",
    "print(resampled_class_counts)\n",
    "print(\"\\nResampled Class Proportions:\")\n",
    "print(resampled_class_proportions)\n",
    "\n",
    "# rf\n",
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_rf_resampled, Y_train_rf_resampled = smote.fit_resample(X_train_rf, Y_train_rf)\n",
    "print(X_train_rf.shape, Y_train_rf.shape, X_train_rf_resampled.shape,Y_train_rf_resampled.shape)\n",
    "\n",
    "#lv\n",
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=55)\n",
    "X_train_lv_resampled, Y_train_lv_resampled = smote.fit_resample(X_train_lv, Y_train_lv)\n",
    "print(X_train_lv.shape, Y_train_lv.shape, X_train_lv_resampled.shape,Y_train_lv_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a3411-e5fc-4d89-bc6a-46fe7a1d1144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression on resampled data\n",
    "\n",
    "# all features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_all_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "logreg_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_logreg_resampled = logreg_all_resampled.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for all features using resampled data:\\n\", classification_report(Y_test, y_pred_logreg_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_resampled = accuracy_score(Y_test, y_pred_logreg_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_resampled = precision_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_resampled = recall_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_resampled = f1_score(Y_test, y_pred_logreg_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for all features using resampled data:\", overall_accuracy_resampled)\n",
    "print(\"Overall Precision for all features using resampled data:\", overall_precision_resampled)\n",
    "print(\"Overall Recall for all features using resampled data:\", overall_recall_resampled)\n",
    "print(\"Overall F1-Score for all features using resampled data:\", overall_f1_score_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_resampled = confusion_matrix(Y_test, y_pred_logreg_resampled)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_resampled = pd.DataFrame(conf_matrix_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for all features using resampled data:\")\n",
    "print(confusion_df_resampled)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_rf\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_rf_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "logreg_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "y_pred_logreg_rf_resampled = logreg_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Random Forest with resampled data:\\n\", classification_report(Y_test_rf, y_pred_logreg_rf_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_rf_resampled = accuracy_score(Y_test_rf, y_pred_logreg_rf_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_rf_resampled = precision_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_rf_resampled = recall_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_rf_resampled = f1_score(Y_test_rf, y_pred_logreg_rf_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Random Forest with resampled data:\", overall_accuracy_rf_resampled)\n",
    "print(\"Overall Precision for selected features using Random Forest with resampled data:\", overall_precision_rf_resampled)\n",
    "print(\"Overall Recall for selected features using Random Forest with resampled data:\", overall_recall_rf_resampled)\n",
    "print(\"Overall F1-Score for selected features using Random Forest with resampled data:\", overall_f1_score_rf_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_rf_resampled = confusion_matrix(Y_test_rf, y_pred_logreg_rf_resampled)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled = pd.DataFrame(conf_matrix_rf_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Random Forest with resampled data:\")\n",
    "print(confusion_df_rf_resampled)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# selected_features_lv\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg_lv_resampled = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "logreg_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "y_pred_logreg_lv_resampled = logreg_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report for selected features using Low Variance with resampled data:\\n\", classification_report(Y_test_lv, y_pred_logreg_lv_resampled))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy_lv_resampled = accuracy_score(Y_test_lv, y_pred_logreg_lv_resampled)\n",
    "\n",
    "# Overall Precision\n",
    "overall_precision_lv_resampled = precision_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Overall Recall\n",
    "overall_recall_lv_resampled = recall_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Overall F1-Score\n",
    "overall_f1_score_lv_resampled = f1_score(Y_test_lv, y_pred_logreg_lv_resampled, average='weighted')\n",
    "\n",
    "# Print overall scores\n",
    "print(\"Overall Accuracy for selected features using Low Variance with resampled data:\", overall_accuracy_lv_resampled)\n",
    "print(\"Overall Precision for selected features using Low Variance with resampled data:\", overall_precision_lv_resampled)\n",
    "print(\"Overall Recall for selected features using Low Variance with resampled data:\", overall_recall_lv_resampled)\n",
    "print(\"Overall F1-Score for selected features using Low Variance with resampled data:\", overall_f1_score_lv_resampled)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix_lv_resampled = confusion_matrix(Y_test_lv, y_pred_logreg_lv_resampled)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled = pd.DataFrame(conf_matrix_lv_resampled, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for selected features using Low Variance with resampled data:\")\n",
    "print(confusion_df_lv_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60199fa4-1dbd-442b-910f-5f37727f84b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model of the above three\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_resampled = {\n",
    "    'all_features_resampled': {'accuracy': overall_accuracy_resampled, 'precision': overall_precision_resampled, \n",
    "                     'recall': overall_recall_resampled, 'f1_score': overall_f1_score_resampled},\n",
    "    'selected_features_rf_resampled': {'accuracy': overall_accuracy_rf_resampled, 'precision': overall_precision_rf_resampled, \n",
    "                             'recall': overall_recall_rf_resampled, 'f1_score': overall_f1_score_rf_resampled},\n",
    "    'selected_features_lv_resampled': {'accuracy': overall_accuracy_lv_resampled, 'precision': overall_precision_lv_resampled, \n",
    "                             'recall': overall_recall_lv_resampled, 'f1_score': overall_f1_score_lv_resampled}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_resampled.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics (you can adjust these weights based on your preference)\n",
    "weights_resampled = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_resampled = {}\n",
    "for model, metrics in models_evaluation_resampled.items():\n",
    "    weighted_sum = sum(metrics[metric] * weights_resampled[metric] for metric in metrics)\n",
    "    weighted_sums_resampled[model] = weighted_sum\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_resampled = sorted(weighted_sums_resampled, key=weighted_sums_resampled.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_resampled, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_resampled[model]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9bd93-7101-4603-965c-b7e364d7373c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Decision Tree on the Resampled Data\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  \n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# all features using resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_all_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "dt_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_dt = dt_all_resampled.predict(X_test)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_dt = accuracy_score(Y_test, Y_pred_all_resampled_dt)\n",
    "precision_all_resampled_dt = precision_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "recall_all_resampled_dt = recall_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "f1_all_resampled_dt = f1_score(Y_test, Y_pred_all_resampled_dt, average='weighted')\n",
    "confusion_matrix_all_resampled_dt = confusion_matrix(Y_test, Y_pred_all_resampled_dt)\n",
    "\n",
    "print(\"For all features using resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_all_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_all_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_dt = pd.DataFrame(confusion_matrix_all_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_dt)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_rf_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "dt_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_dt = dt_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_dt = accuracy_score(Y_test_rf, Y_pred_rf_resampled_dt)\n",
    "precision_rf_resampled_dt = precision_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "recall_rf_resampled_dt = recall_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "f1_rf_resampled_dt = f1_score(Y_test_rf, Y_pred_rf_resampled_dt, average='weighted')\n",
    "confusion_matrix_rf_resampled_dt = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_dt)\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_dt = pd.DataFrame(confusion_matrix_rf_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_rf_resampled_dt)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_lv_resampled = DecisionTreeClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "dt_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_dt = dt_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_dt = accuracy_score(Y_test_lv, Y_pred_lv_resampled_dt)\n",
    "precision_lv_resampled_dt = precision_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "recall_lv_resampled_dt = recall_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "f1_lv_resampled_dt = f1_score(Y_test_lv, Y_pred_lv_resampled_dt, average='weighted')\n",
    "confusion_matrix_lv_resampled_dt = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_dt)\n",
    "\n",
    "print(\"\\nFor selected features using low variance with resampled data (Decision Tree):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_dt))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_dt)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_dt)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_dt)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_dt)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled_dt = pd.DataFrame(confusion_matrix_lv_resampled_dt, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_lv_resampled_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391a46a-7cab-4556-8838-eda6087f80bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model of the above three Decision Tree models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_dt = {\n",
    "    'all_features_dt': {'accuracy': accuracy_all_resampled_dt, 'precision': precision_all_resampled_dt, \n",
    "                        'recall': recall_all_resampled_dt, 'f1_score': f1_all_resampled_dt},\n",
    "    'selected_features_rf_dt': {'accuracy': accuracy_rf_resampled_dt, 'precision': precision_rf_resampled_dt, \n",
    "                                'recall': recall_rf_resampled_dt, 'f1_score': f1_rf_resampled_dt},\n",
    "    'selected_features_lv_dt': {'accuracy': accuracy_lv_resampled_dt, 'precision': precision_lv_resampled_dt, \n",
    "                                'recall': recall_lv_resampled_dt, 'f1_score': f1_lv_resampled_dt}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_dt.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Overall Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Overall Precision:\", metrics['precision'])\n",
    "    print(\"Overall Recall:\", metrics['recall'])\n",
    "    print(\"Overall F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics (you can adjust these weights based on your preference)\n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_dt = {}\n",
    "for model, metrics in models_evaluation_dt.items():\n",
    "    weighted_sum_dt = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_dt[model] = weighted_sum_dt\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_dt = sorted(weighted_sums_dt, key=weighted_sums_dt.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_dt, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_dt[model]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aca3a9-9140-4cd2-aafa-db05cc215ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# All features using resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_all_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "rfc_all_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_rfc = rfc_all_resampled.predict(X_test)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_rfc = accuracy_score(Y_test, Y_pred_all_resampled_rfc)\n",
    "precision_all_resampled_rfc = precision_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "recall_all_resampled_rfc = recall_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "f1_all_resampled_rfc = f1_score(Y_test, Y_pred_all_resampled_rfc, average='weighted')\n",
    "confusion_matrix_all_resampled_rfc = confusion_matrix(Y_test, Y_pred_all_resampled_rfc)\n",
    "\n",
    "print(\"For all features using resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_all_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_all_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_rfc = pd.DataFrame(confusion_matrix_all_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_rfc)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_rf_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "rfc_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_rfc = rfc_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_rfc = accuracy_score(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "precision_rf_resampled_rfc = precision_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "recall_rf_resampled_rfc = recall_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "f1_rf_resampled_rfc = f1_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "confusion_matrix_rf_resampled_rfc = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_rfc)\n",
    "\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for Random Forest with selected features:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_rf_resampled_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances_rf = rfc_rf_resampled.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': X_train_rf_resampled.columns, 'Importance': feature_importances_rf})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance for Random Forest with selected features:\")\n",
    "print(feature_importance_df_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df_rf['Feature'], feature_importance_df_rf['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for Random Forest with selected features:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_rf_resampled_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_lv_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "rfc_lv_resampled.fit(X_train_lv_resampled, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_rfc = rfc_lv_resampled.predict(X_test_lv)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_rfc = accuracy_score(Y_test_lv, Y_pred_lv_resampled_rfc)\n",
    "precision_lv_resampled_rfc = precision_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "recall_lv_resampled_rfc = recall_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "f1_lv_resampled_rfc = f1_score(Y_test_lv, Y_pred_lv_resampled_rfc, average='weighted')\n",
    "confusion_matrix_lv_resampled_rfc = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_rfc)\n",
    "\n",
    "print(\"\\nFor selected features using low variance with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_rfc)\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_lv_resampled_rfc = pd.DataFrame(confusion_matrix_lv_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_lv_resampled_rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4073f9-9112-44c5-aa3f-dbd870254980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model of the above three Random Forest models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_rfc = {\n",
    "    'all_features_rfc': {'accuracy': accuracy_all_resampled_rfc, 'precision': precision_all_resampled_rfc, \n",
    "                         'recall': recall_all_resampled_rfc, 'f1_score': f1_all_resampled_rfc},\n",
    "    'selected_features_rf_rfc': {'accuracy': accuracy_rf_resampled_rfc, 'precision': precision_rf_resampled_rfc, \n",
    "                                 'recall': recall_rf_resampled_rfc, 'f1_score': f1_rf_resampled_rfc},\n",
    "    'selected_features_lv_rfc': {'accuracy': accuracy_lv_resampled_rfc, 'precision': precision_lv_resampled_rfc, \n",
    "                                 'recall': recall_lv_resampled_rfc, 'f1_score': f1_lv_resampled_rfc}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_rfc.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics (you can adjust these weights based on your preference)\n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_rfc = {}\n",
    "for model, metrics in models_evaluation_rfc.items():\n",
    "    weighted_sum_rfc = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_rfc[model] = weighted_sum_rfc\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_rfc = sorted(weighted_sums_rfc, key=weighted_sums_rfc.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_rfc, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_rfc[model]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbe39b-0196-468d-b5be-6e49434f6f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rfc_rf_resampled = RandomForestClassifier(n_estimators=100, random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "rfc_rf_resampled.fit(X_train_rf_resampled, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_rfc = rfc_rf_resampled.predict(X_test_rf)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_rfc = accuracy_score(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "precision_rf_resampled_rfc = precision_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "recall_rf_resampled_rfc = recall_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "f1_rf_resampled_rfc = f1_score(Y_test_rf, Y_pred_rf_resampled_rfc, average='weighted')\n",
    "confusion_matrix_rf_resampled_rfc = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_rfc)\n",
    "\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (Random Forest):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_rfc))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_rfc)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_rfc)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_rfc)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_rfc)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances_rf = rfc_rf_resampled.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': X_train_rf_resampled.columns, 'Importance': feature_importances_rf})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance for Random Forest with selected features:\")\n",
    "print(feature_importance_df_rf)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df_rf['Feature'], feature_importance_df_rf['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_rf_resampled_rfc = pd.DataFrame(confusion_matrix_rf_resampled_rfc, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix for Random Forest with selected features:\")\n",
    "print(confusion_df_rf_resampled_rfc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix_rf_resampled_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Random Forest with selected features')\n",
    "plt.show()\n",
    "\n",
    "# Scores for Random Forest with selected features\n",
    "scores_rf_resampled_rfc = {\n",
    "    'Accuracy': accuracy_rf_resampled_rfc,\n",
    "    'Precision': precision_rf_resampled_rfc,\n",
    "    'Recall': recall_rf_resampled_rfc,\n",
    "    'F1-Score': f1_rf_resampled_rfc\n",
    "}\n",
    "\n",
    "# Plot scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(scores_rf_resampled_rfc.keys(), scores_rf_resampled_rfc.values(), color=['#FF9AA2', '#FFB7B2', '#FFDAC1', '#E2F0CB'])\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Metrics for Random Forest with selected features')\n",
    "plt.ylim(0, 1)  # Limit y-axis to [0, 1] for better visualization of scores\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f46012-0838-4486-9df3-0b339ba5059c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "import numpy as np\n",
    "\n",
    "# Example data for demonstration\n",
    "class_labels = ['Diabetes', 'Not Diabetes']  # Corrected order of class labels\n",
    "confusion_matrix_rf_resampled_rfc = np.array([[8826, 34820], [3621, 3469]])  # Corrected confusion matrix data\n",
    "\n",
    "# Bokeh plot for Confusion Matrix\n",
    "p_confusion_matrix = figure(title=\"Confusion Matrix for Random Forest with selected features\",\n",
    "                            x_range=class_labels, y_range=class_labels[::-1],  # Reverse order for y-axis\n",
    "                            toolbar_location=None, tools=\"\", height=400, width=400)\n",
    "\n",
    "# Transform the data for heatmap\n",
    "confusion_matrix_image = np.array(confusion_matrix_rf_resampled_rfc)\n",
    "\n",
    "# Plot the heatmap\n",
    "for i in range(confusion_matrix_image.shape[0]):\n",
    "    for j in range(confusion_matrix_image.shape[1]):\n",
    "        count = confusion_matrix_image[i, j]\n",
    "        p_confusion_matrix.rect(x=j+0.5, y=i+0.5, width=1, height=1, fill_color='blue', line_color=None)\n",
    "        p_confusion_matrix.text(x=j+0.5, y=i+0.5, text=str(count),\n",
    "                                text_align=\"center\", text_baseline=\"middle\",\n",
    "                                text_color=\"white\", text_font_size=\"12pt\")\n",
    "\n",
    "# Add labels\n",
    "p_confusion_matrix.xaxis.axis_label = 'Predicted Label'\n",
    "p_confusion_matrix.yaxis.axis_label = 'True Label'\n",
    "\n",
    "show(p_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c2cd4-b201-467f-b2b6-979d84eed791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf51f3-a10b-43e4-91fa-3b3fbcfc0fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the order of x-axis categories based on 'General Health' levels\n",
    "order = d_df['GenHlth'].unique()\n",
    "\n",
    "# Calculate the proportion of individuals with diabetes for each combination of 'General Health' and target\n",
    "diabetes_proportion = d_df.groupby(['GenHlth', 'target']).size().unstack(fill_value=0)\n",
    "diabetes_proportion = diabetes_proportion.div(diabetes_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the trend\n",
    "plt.figure(figsize=(10, 6))\n",
    "diabetes_proportion.plot(kind='bar', stacked=True, cmap='viridis', ax=plt.gca())\n",
    "plt.xlabel('General Health')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Proportion of Individuals with Diabetes by General Health and Target')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Target', labels=['Not Have Diabetes', 'Have Diabetes'])\n",
    "plt.ylim(0, 1)  # Ensure y-axis starts from 0 and ends at 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bef3df-9eeb-441d-8a46-1ac8efeec698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import Category10\n",
    "import pandas as pd\n",
    "\n",
    "# Example data for demonstration\n",
    "scores = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [0.758, 0.823, 0.758, 0.783]\n",
    "}\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Define colors using the Category10 palette\n",
    "colors = Category10[4]\n",
    "\n",
    "# Add the 'Color' column to the DataFrame\n",
    "scores_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(scores_df)\n",
    "\n",
    "# Create a figure for model performance metrics\n",
    "p_score = figure(x_range=scores_df['Metric'], height=400, width=500, title=\"Model Performance Metrics\",\n",
    "                 toolbar_location=None, tools=\"\")\n",
    "\n",
    "# Plot the vertical bars with colors referenced from the data source\n",
    "p_score.vbar(x='Metric', top='Score', width=0.9, source=source, \n",
    "             fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Metric\", \"@Metric\"), (\"Score\", \"@Score\")]\n",
    "p_score.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the model performance metrics plot\n",
    "p_score.xaxis.major_label_orientation = 1.2\n",
    "p_score.xgrid.grid_line_color = None\n",
    "p_score.y_range.start = 0\n",
    "p_score.yaxis.axis_label = \"Score\"\n",
    "p_score.xaxis.axis_label = \"Metric\"\n",
    "\n",
    "# Show the plot\n",
    "show(p_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "746d589a-6189-4869-a14e-7f9c7fdf7877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shrin\\\\OneDrive\\\\Desktop\\\\Final Project HDS\\\\bokeh_plots.html'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import column\n",
    "import pandas as pd\n",
    "from bokeh.palettes import Category10\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the feature importance DataFrame\n",
    "feature_importance_data = {\n",
    "    'Feature': ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP'],\n",
    "    'Importance': [0.228, 0.198, 0.159, 0.111, 0.101, 0.078, 0.062, 0.058]\n",
    "}\n",
    "feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "# Assign a color palette to each feature\n",
    "colors = Category10[8]\n",
    "\n",
    "# Round off importance values to three decimals\n",
    "feature_importance_df['Importance'] = feature_importance_df['Importance']\n",
    "\n",
    "# Create a new column for colors in the DataFrame\n",
    "feature_importance_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(feature_importance_df)\n",
    "\n",
    "# Create a figure for feature importance\n",
    "p_importance = figure(x_range=feature_importance_df['Feature'], height=400, width=500, title=\"Random Forest Model Features and Target Results\\n\\nFeature Importance\",\n",
    "                      toolbar_location=None, tools=\"\")\n",
    "p_importance.vbar(x='Feature', top='Importance', width=0.9, source=source, \n",
    "                  fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Feature\", \"@Feature\"), (\"Importance\", \"@Importance\")]\n",
    "p_importance.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the feature importance plot\n",
    "p_importance.xaxis.major_label_orientation = 1.2\n",
    "p_importance.xgrid.grid_line_color = None\n",
    "p_importance.y_range.start = 0\n",
    "p_importance.yaxis.axis_label = \"Importance\"\n",
    "p_importance.xaxis.axis_label = \"Feature\"\n",
    "\n",
    "\n",
    "# Example data for demonstration\n",
    "scores = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [0.758, 0.823, 0.758, 0.783]\n",
    "}\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Define colors using the Category10 palette\n",
    "colors = Category10[4]\n",
    "\n",
    "# Add the 'Color' column to the DataFrame\n",
    "scores_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(scores_df)\n",
    "\n",
    "# Create a figure for model performance metrics\n",
    "p_score = figure(x_range=scores_df['Metric'], height=400, width=500, title=\"Model Performance Metrics\",\n",
    "                 toolbar_location=None, tools=\"\")\n",
    "\n",
    "# Plot the vertical bars with colors referenced from the data source\n",
    "p_score.vbar(x='Metric', top='Score', width=0.9, source=source, \n",
    "             fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Metric\", \"@Metric\"), (\"Score\", \"@Score\")]\n",
    "p_score.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the model performance metrics plot\n",
    "p_score.xaxis.major_label_orientation = 1.2\n",
    "p_score.xgrid.grid_line_color = None\n",
    "p_score.y_range.start = 0\n",
    "p_score.yaxis.axis_label = \"Score\"\n",
    "p_score.xaxis.axis_label = \"Metric\"\n",
    "\n",
    "# Define the list of features\n",
    "features = ['BMI', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP', 'GenHlth']\n",
    "\n",
    "# Create a list to store all the plots\n",
    "plots = []\n",
    "\n",
    "def plot_feature(feature):\n",
    "    if feature == 'BMI':\n",
    "        # Group data by 'target' column\n",
    "        group_0 = d_df[d_df['target'] == 0][feature]\n",
    "        group_1 = d_df[d_df['target'] == 1][feature]\n",
    "\n",
    "        # Create histogram for target 0\n",
    "        hist_0, edges_0 = np.histogram(group_0, bins=20)\n",
    "        p_hist = figure(title=f\"{feature} Distribution\", background_fill_color=\"#fafafa\", height=100, width=200)\n",
    "        p_hist.quad(top=hist_0, bottom=0, left=edges_0[:-1], right=edges_0[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "        # Create histogram for target 1\n",
    "        hist_1, edges_1 = np.histogram(group_1, bins=20)\n",
    "        p_hist.quad(top=hist_1, bottom=0, left=edges_1[:-1], right=edges_1[1:], fill_color=\"red\", line_color=\"white\", alpha=0.5)\n",
    "\n",
    "        # Add hover tooltips\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [(\"Count\", \"@top\")]\n",
    "        p_hist.add_tools(hover)\n",
    "\n",
    "        # Set labels and axis\n",
    "        p_hist.xaxis.axis_label = feature\n",
    "        p_hist.yaxis.axis_label = 'Frequency'\n",
    "\n",
    "        return p_hist\n",
    "\n",
    "    else:\n",
    "        p = figure(x_range=sorted(map(str, d_df[feature].unique()), key=lambda x: int(x)), height=100, width=200,\n",
    "                   title=f'Proportion of Individuals with Diabetes by {feature}',\n",
    "                   toolbar_location=None, tools=\"\")\n",
    "        \n",
    "        # Calculate the proportion of individuals with diabetes for each category of the feature\n",
    "        diabetes_proportion = d_df.groupby([feature, 'target']).size().unstack(fill_value=0)\n",
    "        diabetes_proportion = diabetes_proportion.div(diabetes_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "        # Reset the index to convert hierarchical index to columns\n",
    "        diabetes_proportion.reset_index(inplace=True)\n",
    "\n",
    "        # Convert DataFrame to ColumnDataSource\n",
    "        data = {\n",
    "            'categories': list(map(str, diabetes_proportion[feature])),\n",
    "            'Not Have Diabetes': list(diabetes_proportion[0]),\n",
    "            'Have Diabetes': list(diabetes_proportion[1])\n",
    "        }\n",
    "        source = ColumnDataSource(data=data)\n",
    "\n",
    "        # Plot stacked bars\n",
    "        p.vbar_stack(stackers=['Not Have Diabetes', 'Have Diabetes'], x='categories', width=0.9,\n",
    "                     color=[\"#1f77b4\", \"#ff7f0e\"], source=source,\n",
    "                     legend_label=['Not Have Diabetes', 'Have Diabetes'])\n",
    "\n",
    "        # Add hover tooltips\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [('Proportion', '@$name')]\n",
    "        p.add_tools(hover)\n",
    "\n",
    "        # Set plot properties\n",
    "        p.xaxis.axis_label = feature\n",
    "        p.yaxis.axis_label = 'Proportion'\n",
    "        p.legend.title = 'Diabetes Status'\n",
    "        p.legend.location = 'bottom_right'\n",
    "        p.legend.orientation = 'horizontal'\n",
    "        p.legend.label_text_font_size = '8pt'  # Set the font size of legend labels\n",
    "        p.legend.spacing = 1\n",
    "        return p\n",
    "\n",
    "# Increase the title size for each plot\n",
    "p_importance.title.text_font_size = '18pt'  # Feature Importance plot title size\n",
    "p_score.title.text_font_size = '16pt'\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "# Create plots for each feature and add them to the list\n",
    "plots = [p_importance, p_score]  # Start with the feature importance plot\n",
    "for feature in features:\n",
    "    p = plot_feature(feature)\n",
    "    p.title.text_font_size = '16pt'  # Increase title size for other plots\n",
    "    plots.append(p)\n",
    "\n",
    "# Create a grid layout with two plots in each row\n",
    "plots_grid = [plots[i:i+2] for i in range(0, len(plots), 2)]\n",
    "\n",
    "# Combine the feature importance plot and individual feature plots into one grid\n",
    "grid = gridplot(plots_grid, sizing_mode='scale_width')\n",
    "\n",
    "# Show the grid layout\n",
    "show(grid)\n",
    "\n",
    "from bokeh.plotting import output_file, save\n",
    "\n",
    "# Assuming 'grid' is your grid layout containing all the plots\n",
    "output_file(\"bokeh_plots.html\")\n",
    "save(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d617a5-7ebc-48bf-8409-08e6e817277a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.embed import file_html\n",
    "from bokeh.resources import CDN\n",
    "\n",
    "# Convert the grid layout to HTML\n",
    "html = file_html(grid, CDN, \"Bokeh Plots\")\n",
    "\n",
    "# Display the HTML\n",
    "from IPython.display import HTML\n",
    "HTML(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863416e6-ccbc-4605-abad-e171bfe5e11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.io import export\n",
    "\n",
    "# Create the grid layout\n",
    "grid = gridplot(plots_grid, sizing_mode='scale_width')\n",
    "\n",
    "# Export the grid layout to a PDF file\n",
    "export(grid, filename=\"output.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdab1dd-c1c9-400d-8595-09bcb1d9ebbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import gridplot\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "from bokeh.palettes import Category10\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the feature importance DataFrame\n",
    "feature_importance_data = {\n",
    "    'Feature': ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP'],\n",
    "    'Importance': [0.228096, 0.198901, 0.159836, 0.111624, 0.101331, 0.078566, 0.062965, 0.058681]\n",
    "}\n",
    "feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "# Assign a color palette to each feature\n",
    "colors = Category10[8]\n",
    "\n",
    "# Round off importance values to three decimals\n",
    "feature_importance_df['Importance'] = feature_importance_df['Importance'].round(3)\n",
    "\n",
    "# Create a new column for colors in the DataFrame\n",
    "feature_importance_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(feature_importance_df)\n",
    "\n",
    "# Create a figure for feature importance\n",
    "p_importance = figure(x_range=feature_importance_df['Feature'], height=350, title=\"Feature Importance\",\n",
    "                      toolbar_location=None, tools=\"\")\n",
    "p_importance.vbar(x='Feature', top='Importance', width=0.9, source=source, \n",
    "                  fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Feature\", \"@Feature\"), (\"Importance\", \"@Importance\")]\n",
    "p_importance.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting to the feature importance plot\n",
    "p_importance.xaxis.major_label_orientation = 1.2\n",
    "p_importance.xgrid.grid_line_color = None\n",
    "p_importance.y_range.start = 0\n",
    "p_importance.yaxis.axis_label = \"Importance\"\n",
    "p_importance.xaxis.axis_label = \"Feature\"\n",
    "\n",
    "# Define the list of features\n",
    "features = ['BMI', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP', 'GenHlth']\n",
    "\n",
    "# Create a list to store all the plots\n",
    "plots = []\n",
    "\n",
    "def plot_feature(feature):\n",
    "    if feature == 'BMI':\n",
    "        p_hist = figure(title=f\"{feature} Distribution\", background_fill_color=\"#fafafa\")\n",
    "        hist, edges = np.histogram(d_df[feature], bins=20)\n",
    "        p_hist.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "        p_hist.xaxis.axis_label = feature\n",
    "        p_hist.yaxis.axis_label = 'Frequency'\n",
    "        return p_hist\n",
    "    else:\n",
    "        p = figure(x_range=sorted(map(str, d_df[feature].unique()), key=lambda x: int(x)), height=300, width=300,\n",
    "                   title=f'Proportion of Individuals with Diabetes by {feature}',\n",
    "                   toolbar_location=None, tools=\"\")\n",
    "        \n",
    "        # Calculate the proportion of individuals with diabetes for each category of the feature\n",
    "        diabetes_proportion = d_df.groupby([feature, 'target']).size().unstack(fill_value=0)\n",
    "        diabetes_proportion = diabetes_proportion.div(diabetes_proportion.sum(axis=1), axis=0)\n",
    "\n",
    "        # Reset the index to convert hierarchical index to columns\n",
    "        diabetes_proportion.reset_index(inplace=True)\n",
    "\n",
    "        # Convert DataFrame to ColumnDataSource\n",
    "        data = {\n",
    "            'categories': list(map(str, diabetes_proportion[feature])),\n",
    "            'Not Have Diabetes': list(diabetes_proportion[0]),\n",
    "            'Have Diabetes': list(diabetes_proportion[1])\n",
    "        }\n",
    "        source = ColumnDataSource(data=data)\n",
    "\n",
    "        # Plot stacked bars\n",
    "        colors = cycle([\"#1f77b4\", \"#ff7f0e\"])  # Cycle through different colors for bars\n",
    "        p.vbar_stack(stackers=['Not Have Diabetes', 'Have Diabetes'], x='categories', width=0.9,\n",
    "                     color=[next(colors), next(colors)], source=source,\n",
    "                     legend_label=['Not Have Diabetes', 'Have Diabetes'])\n",
    "\n",
    "        # Add hover tooltips\n",
    "        hover = HoverTool()\n",
    "        hover.tooltips = [('Proportion', '@$name')]\n",
    "        p.add_tools(hover)\n",
    "\n",
    "        # Set plot properties\n",
    "        p.xaxis.axis_label = feature\n",
    "        p.yaxis.axis_label = 'Proportion'\n",
    "        p.legend.title = 'Diabetes Status'\n",
    "        p.legend.location = 'bottom_right'\n",
    "        p.legend.orientation = 'horizontal'\n",
    "        p.legend.label_text_font_size = '6pt'  # Set the font size of legend labels\n",
    "        p.legend.spacing = 1\n",
    "        return p\n",
    "\n",
    "# Create plots for each feature and add them to the list\n",
    "for feature in features:\n",
    "    plots.append(plot_feature(feature))\n",
    "\n",
    "# Reshape the plots list into a 2x5 matrix\n",
    "plots_matrix = [plots[i:i+5] for i in range(0, len(plots), 5)]\n",
    "\n",
    "# Combine the feature importance plot and individual feature plots into one grid\n",
    "grid = gridplot(plots_matrix, sizing_mode='stretch_both')\n",
    "\n",
    "# Show the grid layout\n",
    "show(grid)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6543d-4a27-468b-ab04-7658ebe6dfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Category10\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the feature importance DataFrame\n",
    "feature_importance_data = {\n",
    "    'Feature': ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP'],\n",
    "    'Importance': [0.228096, 0.198901, 0.159836, 0.111624, 0.101331, 0.078566, 0.062965, 0.058681]\n",
    "}\n",
    "feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "# Assign a color palette to each feature\n",
    "colors = Category10[8]\n",
    "\n",
    "# Round off importance values to three decimals\n",
    "feature_importance_df['Importance'] = feature_importance_df['Importance'].round(3)\n",
    "\n",
    "# Create a new column for colors in the DataFrame\n",
    "feature_importance_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource\n",
    "source = ColumnDataSource(feature_importance_df)\n",
    "\n",
    "# Create a figure with title\n",
    "p = figure(x_range=feature_importance_df['Feature'], height=350, title=\"Feature Importance of factors affecting The Target using Random Forest Algo\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "\n",
    "# Plot vertical bars for feature importance with different colors\n",
    "p.vbar(x='Feature', top='Importance', width=0.9, source=source, \n",
    "       fill_color='Color', line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "\n",
    "# Add hover tooltips\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [(\"Feature\", \"@Feature\"), (\"Importance\", \"@Importance\")]\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Add labels and formatting\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.xgrid.grid_line_color = None\n",
    "p.y_range.start = 0\n",
    "p.yaxis.axis_label = \"Importance\"\n",
    "p.xaxis.axis_label = \"Feature\"\n",
    "\n",
    "# Show the plot\n",
    "show(p)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241b4e1-41a8-4899-8490-77425da9c7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.palettes import Category10\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the feature importance DataFrame\n",
    "feature_importance_data = {\n",
    "    'Feature': ['BMI', 'GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education', 'HighBP'],\n",
    "    'Importance': [0.228096, 0.198901, 0.159836, 0.111624, 0.101331, 0.078566, 0.062965, 0.058681]\n",
    "}\n",
    "feature_importance_df = pd.DataFrame(feature_importance_data)\n",
    "\n",
    "# Create the histograms for all features\n",
    "hist_plots = []\n",
    "for feature in feature_importance_df['Feature']:\n",
    "    p_hist = figure(title=f\"{feature} Distribution\", background_fill_color=\"#fafafa\")\n",
    "    hist, edges = np.histogram(d_df[feature], bins=20)\n",
    "    p_hist.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color=\"navy\", line_color=\"white\", alpha=0.5)\n",
    "    p_hist.xaxis.axis_label = feature\n",
    "    p_hist.yaxis.axis_label = 'Frequency'\n",
    "    hist_plots.append(p_hist)\n",
    "\n",
    "# Assign a color palette to each feature for the feature importance plot\n",
    "colors = Category10[8]\n",
    "feature_importance_df['Color'] = colors\n",
    "\n",
    "# Create ColumnDataSource for feature importance plot\n",
    "source = ColumnDataSource(feature_importance_df)\n",
    "\n",
    "# Create the feature importance plot\n",
    "p_importance = figure(x_range=feature_importance_df['Feature'], height=350, title=\"Feature Importance\",\n",
    "                      toolbar_location=None, tools=\"\")\n",
    "p_importance.vbar(x='Feature', top='Importance', width=0.9, source=source, fill_color='Color',\n",
    "                  line_color='white', fill_alpha=0.8, line_alpha=0.8)\n",
    "p_importance.add_tools(HoverTool(tooltips=[(\"Feature\", \"@Feature\"), (\"Importance\", \"@Importance\")]))\n",
    "p_importance.xaxis.major_label_orientation = 1.2\n",
    "p_importance.xgrid.grid_line_color = None\n",
    "p_importance.y_range.start = 0\n",
    "p_importance.yaxis.axis_label = \"Importance\"\n",
    "p_importance.xaxis.axis_label = \"Feature\"\n",
    "\n",
    "# Arrange plots in a grid layout\n",
    "hist_plots.insert(0, p_importance)  # Insert the feature importance plot at the beginning\n",
    "grid = gridplot([[plot for plot in hist_plots]], sizing_mode='stretch_width')\n",
    "\n",
    "# Show the grid layout\n",
    "show(grid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b572c0-037a-4e40-b609-b652865b0d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new variables for resampled datasets\n",
    "X_train_resampled_new = X_train_resampled.copy()\n",
    "X_test_new = X_test.copy()\n",
    "X_train_rf_resampled_new = X_train_rf_resampled.copy()\n",
    "X_test_rf_new = X_test_rf.copy()\n",
    "X_train_lv_resampled_new = X_train_lv_resampled.copy()\n",
    "X_test_lv_new = X_test_lv.copy()\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_columns = ['GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "for col in categorical_columns:\n",
    "    X_train_resampled_new[col] = X_train_resampled_new[col].astype(float)\n",
    "    X_test_new[col] = X_test_new[col].astype(float)\n",
    "    X_train_rf_resampled_new[col] = X_train_rf_resampled_new[col].astype(float)\n",
    "    X_test_rf_new[col] = X_test_rf_new[col].astype(float)\n",
    "    X_train_lv_resampled_new[col] = X_train_lv_resampled_new[col].astype(float)\n",
    "    X_test_lv_new[col] = X_test_lv_new[col].astype(float)\n",
    "\n",
    "# Now you can proceed with training your models using these new variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997da26-4543-4937-950c-36966f62c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# All features using resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_all_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "xgb_all_resampled_new.fit(X_train_resampled_new, Y_train_resampled)\n",
    "\n",
    "# Predict on the testing data\n",
    "Y_pred_all_resampled_xgb_new = xgb_all_resampled_new.predict(X_test_new)\n",
    "\n",
    "# For all features using resampled data\n",
    "accuracy_all_resampled_xgb_new = accuracy_score(Y_test, Y_pred_all_resampled_xgb_new)\n",
    "precision_all_resampled_xgb_new = precision_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "recall_all_resampled_xgb_new = recall_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "f1_all_resampled_xgb_new = f1_score(Y_test, Y_pred_all_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_all_resampled_xgb_new = confusion_matrix(Y_test, Y_pred_all_resampled_xgb_new)\n",
    "\n",
    "print(\"For all features using resampled data (XGBoost with new variables):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_all_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_all_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_all_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_all_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_all_resampled_xgb_new)\n",
    "\n",
    "# Define index and columns labels\n",
    "class_labels = ['Class 0', 'Class 1']  # Update with your actual class labels\n",
    "\n",
    "# Create a DataFrame for the confusion matrix\n",
    "confusion_df_all_resampled_xgb_new = pd.DataFrame(confusion_matrix_all_resampled_xgb_new, index=class_labels, columns=class_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df_all_resampled_xgb_new)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using Random Forest with resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_rf_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "xgb_rf_resampled_new.fit(X_train_rf_resampled_new, Y_train_rf_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_rf_resampled_xgb_new = xgb_rf_resampled_new.predict(X_test_rf_new)\n",
    "\n",
    "# For selected features using Random Forest with resampled data\n",
    "accuracy_rf_resampled_xgb_new = accuracy_score(Y_test_rf, Y_pred_rf_resampled_xgb_new)\n",
    "precision_rf_resampled_xgb_new = precision_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "recall_rf_resampled_xgb_new = recall_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "f1_rf_resampled_xgb_new = f1_score(Y_test_rf, Y_pred_rf_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_rf_resampled_xgb_new = confusion_matrix(Y_test_rf, Y_pred_rf_resampled_xgb_new)\n",
    "\n",
    "# Print metrics for selected features using Random Forest with resampled data\n",
    "print(\"\\nFor selected features using Random Forest with resampled data (XGBoost):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_rf, Y_pred_rf_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_rf_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_rf_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_rf_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_rf_resampled_xgb_new)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix_rf_resampled_xgb_new, index=class_labels, columns=class_labels))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Selected features using low variance with resampled data\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_lv_resampled_new = XGBClassifier(random_state=55)\n",
    "\n",
    "# Train the model on the resampled training data with selected features\n",
    "xgb_lv_resampled_new.fit(X_train_lv_resampled_new, Y_train_lv_resampled)\n",
    "\n",
    "# Predict on the testing data with selected features\n",
    "Y_pred_lv_resampled_xgb_new = xgb_lv_resampled_new.predict(X_test_lv_new)\n",
    "\n",
    "# For selected features using low variance with resampled data\n",
    "accuracy_lv_resampled_xgb_new = accuracy_score(Y_test_lv, Y_pred_lv_resampled_xgb_new)\n",
    "precision_lv_resampled_xgb_new = precision_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "recall_lv_resampled_xgb_new = recall_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "f1_lv_resampled_xgb_new = f1_score(Y_test_lv, Y_pred_lv_resampled_xgb_new, average='weighted')\n",
    "confusion_matrix_lv_resampled_xgb_new = confusion_matrix(Y_test_lv, Y_pred_lv_resampled_xgb_new)\n",
    "\n",
    "# Print metrics for selected features using low variance with resampled data\n",
    "print(\"\\nFor selected features using low variance with resampled data (XGBoost):\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test_lv, Y_pred_lv_resampled_xgb_new))\n",
    "print(\"Overall Accuracy:\", accuracy_lv_resampled_xgb_new)\n",
    "print(\"Overall Precision:\", precision_lv_resampled_xgb_new)\n",
    "print(\"Overall Recall:\", recall_lv_resampled_xgb_new)\n",
    "print(\"Overall F1-Score:\", f1_lv_resampled_xgb_new)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix_lv_resampled_xgb_new, index=class_labels, columns=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f82e1c-a85b-4987-a2a8-6c1ff808c10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best model of the above three XGBoost models\n",
    "\n",
    "# Comparing models based on evaluation metrics\n",
    "\n",
    "# Create a dictionary to store evaluation metrics for each model\n",
    "models_evaluation_xgb = {\n",
    "    'all_features_xgb': {'accuracy': accuracy_all_resampled_xgb_new, 'precision': precision_all_resampled_xgb_new, \n",
    "                         'recall': recall_all_resampled_xgb_new, 'f1_score': f1_all_resampled_xgb_new},\n",
    "    'selected_features_rf_xgb': {'accuracy': accuracy_rf_resampled_xgb_new, 'precision': precision_rf_resampled_xgb_new, \n",
    "                                 'recall': recall_rf_resampled_xgb_new, 'f1_score': f1_rf_resampled_xgb_new},\n",
    "    'selected_features_lv_xgb': {'accuracy': accuracy_lv_resampled_xgb_new, 'precision': precision_lv_resampled_xgb_new, \n",
    "                                 'recall': recall_lv_resampled_xgb_new, 'f1_score': f1_lv_resampled_xgb_new}\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for model, metrics in models_evaluation_xgb.items():\n",
    "    print(f\"\\nEvaluation metrics for {model}:\")\n",
    "    print(\"Accuracy:\", metrics['accuracy'])\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1 Score:\", metrics['f1_score'])\n",
    "\n",
    "# Assign weights to evaluation metrics (you can adjust these weights based on your preference)\n",
    "weights = {'accuracy': 1, 'precision': 1, 'recall': 1, 'f1_score': 1}\n",
    "\n",
    "# Calculate weighted sums for each model\n",
    "weighted_sums_xgb = {}\n",
    "for model, metrics in models_evaluation_xgb.items():\n",
    "    weighted_sum_xgb = sum(metrics[metric] * weights[metric] for metric in metrics)\n",
    "    weighted_sums_xgb[model] = weighted_sum_xgb\n",
    "\n",
    "# Rank the models based on their weighted sums (higher sum is better)\n",
    "ranked_models_xgb = sorted(weighted_sums_xgb, key=weighted_sums_xgb.get, reverse=True)\n",
    "\n",
    "# Print the ranked models\n",
    "print(\"\\nRanked models based on overall performance:\")\n",
    "for i, model in enumerate(ranked_models_xgb, start=1):\n",
    "    print(f\"{i}. {model} (Weighted Sum: {weighted_sums_xgb[model]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd9447-a124-4c0a-bebe-e718ea2ceb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade bokeh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca1cf6-c38f-4472-b286-ade2630b072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade bokeh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592a0a7-9f86-417f-a352-9685134ac437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T test and ANOVA\n",
    "\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "# T-tests\n",
    "t_stat, p_value = ttest_ind(d_df[d_df['HighBP'] == 1]['target'], d_df[d_df['HighBP'] == 0]['target'])\n",
    "\n",
    "# Print T-statistic summary\n",
    "print(\"T-test Summary:\")\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference in the mean values of the target variable (diabetes) between the groups with high blood pressure and without high blood pressure.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in the mean values of the target variable (diabetes) between the groups with high blood pressure and without high blood pressure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d86b1-8b37-441e-8f94-9bf38da8c526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T-test for BMI\n",
    "t_stat_bmi, p_value_bmi = ttest_ind(d_df[d_df['BMI'] >= 25]['target'], d_df[d_df['BMI'] < 25]['target'])\n",
    "\n",
    "# Print T-statistic summary for BMI\n",
    "print(\"T-test Summary for BMI:\")\n",
    "print(f\"T-statistic: {t_stat_bmi}\")\n",
    "print(f\"P-value: {p_value_bmi}\")\n",
    "if p_value_bmi < 0.05:\n",
    "    print(\"There is a statistically significant difference in the mean values of the target variable (diabetes) between individuals with BMI greater than or equal to 25 and those with BMI less than 25.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in the mean values of the target variable (diabetes) between individuals with BMI greater than or equal to 25 and those with BMI less than 25.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf54f4-5930-4405-be9a-bdbd9e56cba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Define a function to perform ANOVA and print the summary\n",
    "def perform_anova(variable_name):\n",
    "    groups = [group['target'] for name, group in d_df.groupby(variable_name)]\n",
    "    f_stat, p_value_anova = f_oneway(*groups)\n",
    "    print(f\"ANOVA Summary for {variable_name}:\")\n",
    "    print(f\"F-statistic: {f_stat}\")\n",
    "    print(f\"P-value: {p_value_anova}\")\n",
    "    if p_value_anova < 0.05:\n",
    "        print(f\"There is a statistically significant difference in the mean values of the target variable (diabetes) across different levels of {variable_name}.\")\n",
    "    else:\n",
    "        print(f\"There is no statistically significant difference in the mean values of the target variable (diabetes) across different levels of {variable_name}.\")\n",
    "\n",
    "# Perform ANOVA for each categorical variable\n",
    "categorical_variables = ['GenHlth', 'Age', 'Income', 'PhysHlth', 'MentHlth', 'Education']\n",
    "for var in categorical_variables:\n",
    "    perform_anova(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ea671-1507-4322-9f34-225c6370a8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.relplot(x=\"BMI\", y=\"Age\", hue=\"Income\", palette=\"muted\", data=d_df)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Physical Health\")\n",
    "plt.title(\"Correlation between Age, Income, and Physical Health\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
